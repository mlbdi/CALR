{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/dataset91/data.py\", dst = \"../working/data.py\")\ncopyfile(src = \"../input/dataset91/utils.py\", dst = \"../working/utils.py\")\ncopyfile(src = \"../input/dataset91/training.py\", dst = \"../working/training.py\")\ncopyfile(src = \"../input/dataset91/layers.py\", dst = \"../working/layers.py\")\n#copyfile(src = \"../input/notes1/weights2.npy\", dst = \"../working/weights2.npy\")\n# import all our functions\n#from data import load_data","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:09.264717Z","iopub.execute_input":"2022-01-17T00:36:09.265191Z","iopub.status.idle":"2022-01-17T00:36:09.343198Z","shell.execute_reply.started":"2022-01-17T00:36:09.265118Z","shell.execute_reply":"2022-01-17T00:36:09.342526Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'../working/layers.py'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install pickle5\nimport pickle5 as pickle\nimport csv\nimport calendar\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import  MultipleLocator, FormatStrFormatter\nfrom scipy.interpolate import splrep\nfrom IPython.core.display import display_html\nfrom keras.models import load_model\nfrom utils import np_haversine, density_map, get_clusters, plot_embeddings\nfrom data import load_data\nfrom training import start_new_session, process_features, create_model\n\n# Display plots inline\n%matplotlib inline\n\n# Fix random seed\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:09.344627Z","iopub.execute_input":"2022-01-17T00:36:09.345082Z","iopub.status.idle":"2022-01-17T00:36:24.750085Z","shell.execute_reply.started":"2022-01-17T00:36:09.345049Z","shell.execute_reply":"2022-01-17T00:36:24.749339Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pickle5\n  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n     |████████████████████████████████| 256 kB 896 kB/s            \n\u001b[?25hInstalling collected packages: pickle5\nSuccessfully installed pickle5-0.0.12\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Estimate the GPS clusters\nprint(\"Estimating clusters...\")\n#clusters = get_clusters(data.train_labels)\nn_epochs=100\nbatch_size=200\nsave_prefix='mymodel'\nconfig=3","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:24.751248Z","iopub.execute_input":"2022-01-17T00:36:24.751504Z","iopub.status.idle":"2022-01-17T00:36:24.760255Z","shell.execute_reply.started":"2022-01-17T00:36:24.751470Z","shell.execute_reply":"2022-01-17T00:36:24.759094Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Estimating clusters...\n","output_type":"stream"}]},{"cell_type":"code","source":"clusters_cache = '/kaggle/input/dataset91/clusters.npy'\nclusters=np.load(clusters_cache, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:24.763131Z","iopub.execute_input":"2022-01-17T00:36:24.763503Z","iopub.status.idle":"2022-01-17T00:36:24.784557Z","shell.execute_reply.started":"2022-01-17T00:36:24.763467Z","shell.execute_reply":"2022-01-17T00:36:24.783939Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"file='/kaggle/input/dataset91/train0.npy'\ntrain0=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train1.npy'\ntrain1=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train2.npy'\ntrain2=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train3.npy'\ntrain3=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train4.npy'\ntrain4=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train5.npy'\ntrain5=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train6.npy'\ntrain6=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\ntrain=[train0,train1,train2,train3,train4,train5,train6]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:24.785773Z","iopub.execute_input":"2022-01-17T00:36:24.786065Z","iopub.status.idle":"2022-01-17T00:36:29.537787Z","shell.execute_reply.started":"2022-01-17T00:36:24.786032Z","shell.execute_reply":"2022-01-17T00:36:29.537044Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"file='/kaggle/input/dataset91/validation0.npy'\nvalidation0=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation1.npy'\nvalidation1=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation2.npy'\nvalidation2=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation3.npy'\nvalidation3=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation4.npy'\nvalidation4=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation5.npy'\nvalidation5=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation6.npy'\nvalidation6=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nvalidation=[validation0,validation1,validation2,validation3,validation4,validation5,validation6]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:29.539172Z","iopub.execute_input":"2022-01-17T00:36:29.539413Z","iopub.status.idle":"2022-01-17T00:36:29.950225Z","shell.execute_reply.started":"2022-01-17T00:36:29.539381Z","shell.execute_reply":"2022-01-17T00:36:29.949385Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"TrajLSTM-LocationAttention","metadata":{}},{"cell_type":"markdown","source":"Fitur","metadata":{}},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import scale\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import SGD, Adam, Adagrad\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.core import Dense, Reshape, Activation, Dropout\nfrom keras.layers import *\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom utils import tf_haversine\nfrom data import load_data\nfrom utils import get_clusters\nfrom keras.models import Model\n\nfrom tensorflow.compat.v1.keras.layers import LSTM   # CuDNNLSTM\nimport sys\nsys.path.append('..')  # add parent directory to Python path for layers.py access\nfrom layers import Attention, SelfAttention\n\ndef create_model(metadata,clusters,config):\n    \"\"\"\n    Creates all the layers for our neural network model.\n    \"\"\"\n      \n    # Arbitrary dimension for all embeddings\n    embedding_dim = 10\n\n    # Quarter hour of the day embedding\n    embed_quarter_hour = Sequential()\n    embed_quarter_hour.add(Embedding(metadata['n_quarter_hours'], embedding_dim, input_length=1))\n    embed_quarter_hour.add(Reshape((embedding_dim,1)))\n\n    # Day of the week embedding\n    embed_day_of_week = Sequential()\n    embed_day_of_week.add(Embedding(metadata['n_days_per_week'], embedding_dim, input_length=1))\n    embed_day_of_week.add(Reshape((embedding_dim,1)))\n\n    # Week of the year embedding\n    embed_week_of_year = Sequential()\n    embed_week_of_year.add(Embedding(metadata['n_weeks_per_year'], embedding_dim, input_length=1))\n    embed_week_of_year.add(Reshape((embedding_dim,1)))\n\n    # Client ID embedding\n    embed_client_ids = Sequential()\n    embed_client_ids.add(Embedding(metadata['n_client_ids'], embedding_dim, input_length=1))\n    embed_client_ids.add(Reshape((embedding_dim,1)))\n\n    # Taxi ID embedding\n    embed_taxi_ids = Sequential()\n    embed_taxi_ids.add(Embedding(metadata['n_taxi_ids'], embedding_dim, input_length=1))\n    embed_taxi_ids.add(Reshape((embedding_dim,1)))\n\n    # Taxi stand ID embedding\n    embed_stand_ids = Sequential()\n    embed_stand_ids.add(Embedding(metadata['n_stand_ids'], embedding_dim, input_length=1))\n    embed_stand_ids.add(Reshape((embedding_dim,1)))\n    \n    # GPS coordinates (5 first lat/long and 5 latest lat/long, therefore 20 values)\n    coords = Sequential()\n    coords.add(Dense(1, input_dim=20))\n\n    # Merge all the inputs into a single input layer\n    mergedOut = Add()([embed_quarter_hour.output,\n                embed_day_of_week.output,\n                embed_week_of_year.output,\n                embed_client_ids.output,\n                embed_taxi_ids.output,\n                embed_stand_ids.output,\n                coords.output])\n    \n    #encoder_output, hidden_state, cell_state = LSTM(50, activation='tanh',input_shape=(None,None, 1),\n    #                                                return_sequences=True,return_state=True)(mergedOut)\n    #attention_input = [encoder_output, hidden_state]\n    lstm_layer = tf.keras.layers.LSTM(200, return_sequences=True,return_state=True)\n    if config != 0:\n        encoder_output, hidden_state, cell_state,  *_ = Bidirectional(lstm_layer,merge_mode=\"sum\")(mergedOut)\n        attention_input = [encoder_output, hidden_state]\n    else:\n        encoder_output = LSTM(units=64)(mergedOut)\n\n    # Optional Attention Mechanisms\n    if config == 1:\n        encoder_output, attention_weights = SelfAttention(size=64,\n                                                      num_hops=10,\n                                                      use_penalization=False)(encoder_output)\n    elif config == 2:\n        encoder_output, attention_weights = Attention(context='many-to-one',\n                                                  alignment_type='global')(attention_input)\n        encoder_output = Flatten()(encoder_output)\n    elif config == 3:\n        encoder_output, attention_weights = Attention(context='many-to-one',\n                                                  alignment_type='local-m',\n                                                  window_width=10,\n                                                  score_function='general')(attention_input)\n        encoder_output = Flatten()(encoder_output)\n    \n    #encoder_output=Dense(1)(encoder_output)\n    # Determine cluster probabilities using softmax\n    mergedOut=Dense(len(clusters))(encoder_output)\n    mergedOut=Activation('softmax')(mergedOut)\n\n    # Final activation layer: calculate the destination as the weighted mean of cluster coordinates\n    cast_clusters = K.cast_to_floatx(clusters)\n    def destination(probabilities):\n        return tf.matmul(probabilities, cast_clusters)\n    mergedOut=Activation(destination)(mergedOut)\n\n    newModel = Model([embed_quarter_hour.input,\n                embed_day_of_week.input,\n                embed_week_of_year.input,\n                embed_client_ids.input,\n                embed_taxi_ids.input,\n                embed_stand_ids.input,\n                coords.input], mergedOut)\n    #use lists if you want more than one input or output  \n    \n    # Compile the model\n    optimizer = SGD(lr=0.1, momentum=0.9, clipvalue=1.)  # Use `clipvalue` to prevent exploding gradients\n\n    newModel.compile(loss=tf_haversine, optimizer=optimizer)\n    \n    return newModel","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:29.953497Z","iopub.execute_input":"2022-01-17T00:36:29.953922Z","iopub.status.idle":"2022-01-17T00:36:30.073091Z","shell.execute_reply.started":"2022-01-17T00:36:29.953886Z","shell.execute_reply":"2022-01-17T00:36:30.072443Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"GPS","metadata":{}},{"cell_type":"markdown","source":"Create Model dan Training","metadata":{}},{"cell_type":"code","source":"import pickle5 as pickle\nmetadata_cache = '/kaggle/input/dataset91/metadata.pickle'\nwith open(metadata_cache, 'rb') as handle:\n    metadata = pickle.load(handle)\ntl_cache = '/kaggle/input/dataset91/train-labels.npy'\nvl_cache = '/kaggle/input/dataset91/validation-labels.npy'\ntrain_labels=np.load(tl_cache, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')\nvalidation_labels=np.load(vl_cache, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:30.074089Z","iopub.execute_input":"2022-01-17T00:36:30.074314Z","iopub.status.idle":"2022-01-17T00:36:30.425249Z","shell.execute_reply.started":"2022-01-17T00:36:30.074282Z","shell.execute_reply":"2022-01-17T00:36:30.424509Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"config=3\nmodel = create_model(metadata,clusters,config)\n#c=0","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:30.427795Z","iopub.execute_input":"2022-01-17T00:36:30.428043Z","iopub.status.idle":"2022-01-17T00:36:34.472718Z","shell.execute_reply.started":"2022-01-17T00:36:30.428009Z","shell.execute_reply":"2022-01-17T00:36:34.470971Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2022-01-17 00:36:30.499707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:30.603943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:30.604666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:30.605895: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-01-17 00:36:30.606651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:30.607640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:30.608649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:32.274736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:32.275550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:32.276248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:32.276834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Parameter","metadata":{}},{"cell_type":"markdown","source":"Custom Callback","metadata":{}},{"cell_type":"code","source":"e=[]\nf=[]\ng=[]\nfor i in range(60):\n    f.append(0)\n    e.append(0)\n    g.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:34.474008Z","iopub.execute_input":"2022-01-17T00:36:34.474238Z","iopub.status.idle":"2022-01-17T00:36:34.479273Z","shell.execute_reply.started":"2022-01-17T00:36:34.474205Z","shell.execute_reply":"2022-01-17T00:36:34.478204Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class MyCallback(tf.keras.callbacks.Callback):\n    def on_train_end(self, logs=None):\n        global training_finished\n        training_finished = True\n    def on_epoch_end(self, epoch, logs=None):\n        z=len(e)\n        if z>len(g):\n            a=model.history.history['val_loss'][z-len(g)-1]\n            b=model.history.history['loss'][z-len(g)-1]\n            c=float(a)-float(b)\n            if c<0:\n                print(\"Sebelumnya Underfitting\")\n            else:\n                print(\"Sebelumnya Overfitting\")\n        e.append(0)\n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:34.480655Z","iopub.execute_input":"2022-01-17T00:36:34.481013Z","iopub.status.idle":"2022-01-17T00:36:34.490369Z","shell.execute_reply.started":"2022-01-17T00:36:34.480979Z","shell.execute_reply":"2022-01-17T00:36:34.489621Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class CustCallback(tf.keras.callbacks.Callback):\n    def on_train_end(self, logs=None):\n        global training_finished\n        training_finished = True\n    def on_epoch_end(self, epoch, logs=None):\n        z=len(f)\n        file_path=\"./weights\"+str(z)\n        weight=model.get_weights()\n        np.save(file_path, weight)\n        f.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:34.491841Z","iopub.execute_input":"2022-01-17T00:36:34.492885Z","iopub.status.idle":"2022-01-17T00:36:34.502170Z","shell.execute_reply.started":"2022-01-17T00:36:34.492849Z","shell.execute_reply":"2022-01-17T00:36:34.501421Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Load Saved Weight","metadata":{}},{"cell_type":"code","source":"file='../input/latb91fiturlocm/weights'+str(len(g)-1)+'.npy'\n#file='./weights'+str(len(g)-1)+'.npy'\ny=np.load(file,allow_pickle=True)\nmodel.set_weights(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:34.503329Z","iopub.execute_input":"2022-01-17T00:36:34.503709Z","iopub.status.idle":"2022-01-17T00:36:36.320950Z","shell.execute_reply.started":"2022-01-17T00:36:34.503672Z","shell.execute_reply":"2022-01-17T00:36:36.320190Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, CSVLogger\nfrom keras.callbacks import EarlyStopping\n\n#g.append(0)\n#e.append(0)\narc='LSTMnew91-b200-'+str(len(g))+'.log'\ncsv_logger = CSVLogger(arc, separator=',', append=False)\nprint(\"Creating model...\")\nstart_new_session()\nprint(\"Train model...\")\nhistory=model.fit(train, train_labels,\n        initial_epoch=len(g),epochs=100, batch_size=batch_size,\n        validation_data=(validation, validation_labels)\n        ,callbacks=[csv_logger,CustCallback(),MyCallback()])\n\nT=True\nwhile(T):\n    T=True","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:36:36.324043Z","iopub.execute_input":"2022-01-17T00:36:36.324247Z","iopub.status.idle":"2022-01-17T02:07:10.092052Z","shell.execute_reply.started":"2022-01-17T00:36:36.324222Z","shell.execute_reply":"2022-01-17T02:07:10.040982Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Creating model...\n","output_type":"stream"},{"name":"stderr","text":"2022-01-17 00:36:36.328249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:36.331284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:36.331947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:36.332697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:36.333392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-17 00:36:36.333874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Train model...\n","output_type":"stream"},{"name":"stderr","text":"2022-01-17 00:36:37.077386: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61/100\n","output_type":"stream"},{"name":"stderr","text":"2022-01-17 00:36:40.534706: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla P100-PCIE-16GB\" frequency: 1328 num_cores: 56 environment { key: \"architecture\" value: \"6.0\" } environment { key: \"cuda\" value: \"11000\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 16152002560 bandwidth: 732160000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n2022-01-17 00:36:42.204279: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"7398/7400 [============================>.] - ETA: 0s - loss: 1.9219","output_type":"stream"},{"name":"stderr","text":"2022-01-17 00:38:12.975913: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla P100-PCIE-16GB\" frequency: 1328 num_cores: 56 environment { key: \"architecture\" value: \"6.0\" } environment { key: \"cuda\" value: \"11000\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 16152002560 bandwidth: 732160000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n","output_type":"stream"},{"name":"stdout","text":"7400/7400 [==============================] - 100s 13ms/step - loss: 1.9219 - val_loss: 2.0065\nEpoch 62/100\n  11/7400 [..............................] - ETA: 1:28 - loss: 1.9553","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order, subok=True)\n","output_type":"stream"},{"name":"stdout","text":"7400/7400 [==============================] - 93s 13ms/step - loss: 1.9210 - val_loss: 2.0028\nSebelumnya Overfitting\nEpoch 63/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9205 - val_loss: 2.0049\nSebelumnya Overfitting\nEpoch 64/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9200 - val_loss: 2.0079\nSebelumnya Overfitting\nEpoch 65/100\n7400/7400 [==============================] - 93s 13ms/step - loss: 1.9186 - val_loss: 2.0079\nSebelumnya Overfitting\nEpoch 66/100\n7400/7400 [==============================] - 93s 13ms/step - loss: 1.9167 - val_loss: 2.0082\nSebelumnya Overfitting\nEpoch 67/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9170 - val_loss: 2.0275\nSebelumnya Overfitting\nEpoch 68/100\n7400/7400 [==============================] - 92s 12ms/step - loss: 1.9143 - val_loss: 2.0130\nSebelumnya Overfitting\nEpoch 69/100\n7400/7400 [==============================] - 93s 13ms/step - loss: 1.9138 - val_loss: 2.0019\nSebelumnya Overfitting\nEpoch 70/100\n7400/7400 [==============================] - 93s 13ms/step - loss: 1.9126 - val_loss: 2.0071\nSebelumnya Overfitting\nEpoch 71/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9121 - val_loss: 2.0087\nSebelumnya Overfitting\nEpoch 72/100\n7400/7400 [==============================] - 93s 13ms/step - loss: 1.9099 - val_loss: 2.0086\nSebelumnya Overfitting\nEpoch 73/100\n7400/7400 [==============================] - 93s 13ms/step - loss: 1.9094 - val_loss: 2.0062\nSebelumnya Overfitting\nEpoch 74/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9077 - val_loss: 2.0046\nSebelumnya Overfitting\nEpoch 75/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9065 - val_loss: 2.0085\nSebelumnya Overfitting\nEpoch 76/100\n7400/7400 [==============================] - 93s 13ms/step - loss: 1.9077 - val_loss: 2.0081\nSebelumnya Overfitting\nEpoch 77/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9056 - val_loss: 2.0108\nSebelumnya Overfitting\nEpoch 78/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9040 - val_loss: 2.0064\nSebelumnya Overfitting\nEpoch 79/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9033 - val_loss: 2.0100\nSebelumnya Overfitting\nEpoch 80/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9021 - val_loss: 2.0126\nSebelumnya Overfitting\nEpoch 81/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.9002 - val_loss: 2.0280\nSebelumnya Overfitting\nEpoch 82/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.8987 - val_loss: 2.0018\nSebelumnya Overfitting\nEpoch 83/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.8982 - val_loss: 2.0151\nSebelumnya Overfitting\nEpoch 84/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.8987 - val_loss: 2.0321\nSebelumnya Overfitting\nEpoch 85/100\n7400/7400 [==============================] - 94s 13ms/step - loss: 1.8955 - val_loss: 2.0078\nSebelumnya Overfitting\nEpoch 86/100\n7400/7400 [==============================] - 95s 13ms/step - loss: 1.8949 - val_loss: 2.0071\nSebelumnya Overfitting\nEpoch 87/100\n7400/7400 [==============================] - 95s 13ms/step - loss: 1.8916 - val_loss: 2.0133\nSebelumnya Overfitting\nEpoch 88/100\n7400/7400 [==============================] - 95s 13ms/step - loss: 1.8935 - val_loss: 2.0087\nSebelumnya Overfitting\nEpoch 89/100\n7400/7400 [==============================] - 95s 13ms/step - loss: 1.8924 - val_loss: 2.0056\nSebelumnya Overfitting\nEpoch 90/100\n7400/7400 [==============================] - 95s 13ms/step - loss: 1.8901 - val_loss: 2.0091\nSebelumnya Overfitting\nEpoch 91/100\n7400/7400 [==============================] - 95s 13ms/step - loss: 1.8889 - val_loss: 2.0116\nSebelumnya Overfitting\nEpoch 92/100\n7400/7400 [==============================] - 97s 13ms/step - loss: 1.8876 - val_loss: 2.0076\nSebelumnya Overfitting\nEpoch 93/100\n7400/7400 [==============================] - 104s 14ms/step - loss: 1.8851 - val_loss: 2.0063\nSebelumnya Overfitting\nEpoch 94/100\n7400/7400 [==============================] - 109s 15ms/step - loss: 1.8865 - val_loss: 2.0134\nSebelumnya Overfitting\nEpoch 95/100\n7400/7400 [==============================] - 115s 16ms/step - loss: 1.8847 - val_loss: 2.0136\nSebelumnya Overfitting\nEpoch 96/100\n7400/7400 [==============================] - 123s 17ms/step - loss: 1.8850 - val_loss: 2.0150\nSebelumnya Overfitting\nEpoch 97/100\n7400/7400 [==============================] - 130s 18ms/step - loss: 1.8831 - val_loss: 2.0109\nSebelumnya Overfitting\nEpoch 98/100\n7400/7400 [==============================] - 131s 18ms/step - loss: 1.8806 - val_loss: 2.0157\nSebelumnya Overfitting\nEpoch 99/100\n7400/7400 [==============================] - 132s 18ms/step - loss: 1.8789 - val_loss: 2.0116\nSebelumnya Overfitting\nEpoch 100/100\n7400/7400 [==============================] - 133s 18ms/step - loss: 1.8789 - val_loss: 2.0126\nSebelumnya Overfitting\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_53/993318234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#metadata_cache = '/kaggle/input/dataset91/train.pickle'\n#with open(metadata_cache, 'rb') as handle:\n#    train = pickle.load(handle)\n#metadata_cache = '/kaggle/input/dataset91/validation.pickle'\n#with open(metadata_cache, 'rb') as handle:\n#    validation = pickle.load(handle)\nimport pickle5 as pickle\nmetadata_cache = '/kaggle/input/dataset91/competition-test.pickle'\nwith open(metadata_cache, 'rb') as handle:\n    competition_test = pickle.load(handle)\ntrain_labels=np.load('/kaggle/input/dataset91/train-labels.npy')\nvalidation_labels=np.load('/kaggle/input/dataset91/validation-labels.npy')\ncompetition_test_labels=np.load('/kaggle/input/dataset91/competition-test-labels.npy')","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:07:10.093150Z","iopub.status.idle":"2022-01-17T02:07:10.093574Z","shell.execute_reply.started":"2022-01-17T02:07:10.093341Z","shell.execute_reply":"2022-01-17T02:07:10.093363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"HASIL EVALUASI MODEL-INPUT 82 Full GPS\")\nprint(\" \")\ntest_predictions = model.predict(process_features(competition_test))\nprint(\"Mean Haversine Test:{}\".format(np_haversine(test_predictions, competition_test_labels).mean()))\nvalidation_predictions = model.predict(validation)\nprint(\"Mean Haversine Validation:{}\".format(np_haversine(validation_predictions, validation_labels).mean()))\ntrain_predictions = model.predict(train)\nprint(\"Mean Haversine Train:{}\".format(np_haversine(train_predictions, train_labels).mean()))\nprint(\" \")\ny_train=train_labels\ny_train_pred=train_predictions\ny_valid=validation_labels\ny_valid_pred=validation_predictions\ny_test=competition_test_labels\ny_test_pred=test_predictions\nfrom sklearn import metrics\nprint(\"MSE train:{}\".format(metrics.mean_squared_error(y_train, y_train_pred)))\nprint(\"MSE validation:{}\".format(metrics.mean_squared_error(y_valid, y_valid_pred)))\nprint(\"MSE test:{}\".format(metrics.mean_squared_error(y_test, y_test_pred)))\nprint(\" \")\nprint(\"MAE score train:{}\".format(metrics.mean_absolute_error(y_train, y_train_pred)))\nprint(\"MAE score validation:{}\".format(metrics.mean_absolute_error(y_valid, y_valid_pred)))\nprint(\"MAE score test:{}\".format(metrics.mean_absolute_error(y_test, y_test_pred)))\nprint(\" \")\nprint(\"R2 score train:{}\".format(metrics.r2_score(y_train, y_train_pred)))\nprint(\"R2 score validation:{}\".format(metrics.r2_score(y_valid, y_valid_pred)))\nprint(\"R2 score test:{}\".format(metrics.r2_score(y_test, y_test_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T02:07:10.094843Z","iopub.status.idle":"2022-01-17T02:07:10.095369Z","shell.execute_reply.started":"2022-01-17T02:07:10.095148Z","shell.execute_reply":"2022-01-17T02:07:10.095171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EVALUASI","metadata":{}},{"cell_type":"markdown","source":"FITUR","metadata":{}}]}