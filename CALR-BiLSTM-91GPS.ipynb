{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/dataset91/data.py\", dst = \"../working/data.py\")\ncopyfile(src = \"../input/dataset91/utils.py\", dst = \"../working/utils.py\")\ncopyfile(src = \"../input/dataset91/training.py\", dst = \"../working/training.py\")\ncopyfile(src = \"../input/dataset91/layers.py\", dst = \"../working/layers.py\")\n#copyfile(src = \"../input/notes1/weights2.npy\", dst = \"../working/weights2.npy\")\n# import all our functions\n#from data import load_data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'../working/layers.py'"},"metadata":{}}]},{"cell_type":"code","source":"print(2)","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Estimate the GPS clusters\nprint(\"Estimating clusters...\")\n#clusters = get_clusters(data.train_labels)\nn_epochs=100\nbatch_size=200\nsave_prefix='mymodel'\nconfig=3\n","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Estimating clusters...\n","output_type":"stream"}]},{"cell_type":"code","source":"#import pickle\n!pip install pickle5\nimport csv\nimport calendar\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import  MultipleLocator, FormatStrFormatter\nfrom scipy.interpolate import splrep\nfrom IPython.core.display import display_html\nfrom keras.models import load_model\nfrom utils import np_haversine, density_map, get_clusters, plot_embeddings\nfrom data import load_data\n#from training import start_new_session, process_features, create_model\n\n# Display plots inline\n%matplotlib inline\n\n# Fix random seed for reproducibility\nnp.random.seed(42)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pickle5\n  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n     |████████████████████████████████| 256 kB 5.1 MB/s            \n\u001b[?25hInstalling collected packages: pickle5\nSuccessfully installed pickle5-0.0.12\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"def start_new_session():\n    \"\"\"\n    Starts a new Tensorflow session.\n    \"\"\"\n    \n    # Make sure the session only uses the GPU memory that it actually needs\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    \n    session = tf.compat.v1.Session(config=config, graph=tf.compat.v1.get_default_graph())\n    tf.compat.v1.keras.backend.set_session(session)","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"clusters_cache = '/kaggle/input/dataset91/clusters.npy'\nclusters=np.load(clusters_cache, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"file='/kaggle/input/dataset91/train0.npy'\ntrain0=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train1.npy'\ntrain1=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train2.npy'\ntrain2=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train3.npy'\ntrain3=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train4.npy'\ntrain4=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train5.npy'\ntrain5=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/train6.npy'\ntrain6=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\ntrain=[train0,train1,train2,train3,train4,train5,train6]","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"file='/kaggle/input/dataset91/validation0.npy'\nvalidation0=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation1.npy'\nvalidation1=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation2.npy'\nvalidation2=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation3.npy'\nvalidation3=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation4.npy'\nvalidation4=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation5.npy'\nvalidation5=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/dataset91/validation6.npy'\nvalidation6=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nvalidation=[validation0,validation1,validation2,validation3,validation4,validation5,validation6]","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#!pip3 install pickle5\nimport pickle5 as pickle\nmetadata_cache = '/kaggle/input/dataset91/metadata.pickle'\nwith open(metadata_cache, 'rb') as handle:\n    metadata = pickle.load(handle)\ntl_cache = '/kaggle/input/dataset91/train-labels.npy'\nvl_cache = '/kaggle/input/dataset91/validation-labels.npy'\ntrain_labels=np.load(tl_cache, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')\nvalidation_labels=np.load(vl_cache, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train=train[6]\nvalidation=validation[6]","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def first_last_k(coords):\n    \"\"\"\n    Returns a list with the first k and last k GPS coordinates from the given trip.\n    The returned list contains 4k values (latitudes and longitudes for 2k points).\n    \"\"\"\n    k = 5\n    partial = [coords[0] for i in range(2*k)]\n    num_coords = len(coords)\n    if num_coords < 2*k:\n        partial[-num_coords:] = coords\n    else:\n        partial[:k] = coords[:k]\n        partial[-k:] = coords[-k:]\n    partial = np.row_stack(partial)\n    return np.array(partial).flatten()\n\n\ndef process_features(df):\n    \"\"\"\n    Process the features required by our model from the given dataframe.\n    Return the features in a list so that they can be merged in our model's input layer.\n    \"\"\"\n    # Fetch the first and last GPS coordinates\n    coords = np.row_stack(df['POLYLINE'].apply(first_last_k))\n    # Standardize latitudes (odd columns) and longitudes (even columns)\n    latitudes = coords[:,::2]\n    coords[:,::2] = scale(latitudes)\n    longitudes = coords[:,1::2]\n    coords[:,1::2] = scale(longitudes)\n    \n    return [\n        coords\n    ]\n\n\nfrom tensorflow.compat.v1.keras.layers import LSTM   # CuDNNLSTM\nimport sys\nsys.path.append('..')  # add parent directory to Python path for layers.py access\nfrom layers import Attention, SelfAttention\n\n#trayektori\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import scale\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import SGD, Adam, Adagrad\nimport numpy as np\nfrom keras.layers.core import Dense, Reshape, Activation, Dropout\nfrom keras.layers import *\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom utils import tf_haversine\n\nfrom utils import get_clusters\nfrom keras.models import Model\n\ndef create_model(metadata,clusters,config):\n    from layers import Attention, SelfAttention\n    \"\"\"\n    Creates all the layers for our neural network model.\n    \"\"\"\n      \n    # Arbitrary dimension for all embeddings\n    embedding_dim = 10\n\n    # GPS coordinates (5 first lat/long and 5 latest lat/long, therefore 20 values)\n    coords = Sequential()\n    coords.add(Dense(1, input_dim=20,input_shape=(20,1)))\n    \n    #np.expand_dims(coords,axis=0)\n    mergedOut = coords.output\n    lstm_layer = tf.keras.layers.LSTM(200, return_sequences=True,return_state=True)\n    if config != 0:\n        encoder_output, hidden_state, cell_state,  *_ = Bidirectional(lstm_layer,merge_mode=\"sum\")(mergedOut)\n        attention_input = [encoder_output, hidden_state]\n    else:\n        encoder_output = LSTM(units=64)(mergedOut)\n\n    # Optional Attention Mechanisms\n    if config == 1:\n        encoder_output, attention_weights = SelfAttention(size=64,\n                                                      num_hops=10,\n                                                      use_penalization=False)(encoder_output)\n    elif config == 2:\n        encoder_output, attention_weights = Attention(context='many-to-one',\n                                                  alignment_type='global')(attention_input)\n        encoder_output = Flatten()(encoder_output)\n    elif config == 3:\n        encoder_output, attention_weights = Attention(context='many-to-one',\n                                                  alignment_type='local-m',\n                                                  window_width=10,\n                                                  score_function='general')(attention_input)\n        encoder_output = Flatten()(encoder_output)\n        \n    \n    #encoder_output=Dense(1)(encoder_output)\n    # Determine cluster probabilities using softmax\n    mergedOut=Dense(len(clusters))(encoder_output)\n    mergedOut=Activation('softmax')(mergedOut)\n\n    # Final activation layer: calculate the destination as the weighted mean of cluster coordinates\n    cast_clusters = K.cast_to_floatx(clusters)\n    def destination(probabilities):\n        return tf.matmul(probabilities, cast_clusters)\n    mergedOut=Activation(destination)(mergedOut)\n\n    newModel = Model([coords.input],mergedOut)\n    #use lists if you want more than one input or output  \n    \n    # Compile the model\n    optimizer = SGD(lr=0.01, momentum=0.9, clipvalue=1.)  # Use `clipvalue` to prevent exploding gradients\n\n    newModel.compile(loss=tf_haversine, optimizer=optimizer)\n    \n    return newModel","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model=create_model(metadata,clusters,config)","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\nUser settings:\n\n   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n   KMP_BLOCKTIME=0\n   KMP_DUPLICATE_LIB_OK=True\n   KMP_INIT_AT_FORK=FALSE\n   KMP_SETTINGS=1\n   KMP_WARNINGS=0\n\nEffective settings:\n\n   KMP_ABORT_DELAY=0\n   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n   KMP_ALIGN_ALLOC=64\n   KMP_ALL_THREADPRIVATE=128\n   KMP_ATOMIC_MODE=2\n   KMP_BLOCKTIME=0\n   KMP_CPUINFO_FILE: value is not defined\n   KMP_DETERMINISTIC_REDUCTION=false\n   KMP_DEVICE_THREAD_LIMIT=2147483647\n   KMP_DISP_NUM_BUFFERS=7\n   KMP_DUPLICATE_LIB_OK=true\n   KMP_ENABLE_TASK_THROTTLING=true\n   KMP_FORCE_REDUCTION: value is not defined\n   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n   KMP_FORKJOIN_BARRIER='2,2'\n   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n   KMP_GTID_MODE=3\n   KMP_HANDLE_SIGNALS=false\n   KMP_HOT_TEAMS_MAX_LEVEL=1\n   KMP_HOT_TEAMS_MODE=0\n   KMP_INIT_AT_FORK=true\n   KMP_LIBRARY=throughput\n   KMP_LOCK_KIND=queuing\n   KMP_MALLOC_POOL_INCR=1M\n   KMP_NUM_LOCKS_IN_BLOCK=1\n   KMP_PLAIN_BARRIER='2,2'\n   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n   KMP_REDUCTION_BARRIER='1,1'\n   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n   KMP_SCHEDULE='static,balanced;guided,iterative'\n   KMP_SETTINGS=true\n   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n   KMP_STACKOFFSET=64\n   KMP_STACKPAD=0\n   KMP_STACKSIZE=8M\n   KMP_STORAGE_MAP=false\n   KMP_TASKING=2\n   KMP_TASKLOOP_MIN_TASKS=0\n   KMP_TASK_STEALING_CONSTRAINT=1\n   KMP_TEAMS_THREAD_LIMIT=4\n   KMP_TOPOLOGY_METHOD=all\n   KMP_USE_YIELD=1\n   KMP_VERSION=false\n   KMP_WARNINGS=false\n   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n   OMP_ALLOCATOR=omp_default_mem_alloc\n   OMP_CANCELLATION=false\n   OMP_DEFAULT_DEVICE=0\n   OMP_DISPLAY_AFFINITY=false\n   OMP_DISPLAY_ENV=false\n   OMP_DYNAMIC=false\n   OMP_MAX_ACTIVE_LEVELS=1\n   OMP_MAX_TASK_PRIORITY=0\n   OMP_NESTED: deprecated; max-active-levels-var=1\n   OMP_NUM_THREADS: value is not defined\n   OMP_PLACES: value is not defined\n   OMP_PROC_BIND='intel'\n   OMP_SCHEDULE='static'\n   OMP_STACKSIZE=8M\n   OMP_TARGET_OFFLOAD=DEFAULT\n   OMP_THREAD_LIMIT=2147483647\n   OMP_WAIT_POLICY=PASSIVE\n   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'\n\n2022-01-05 16:19:20.112475: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ndense_input (InputLayer)        [(None, 20, 1)]      0                                            \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 20, 1)        2           dense_input[0][0]                \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   [(None, 20, 200), (N 323200      dense[0][0]                      \n__________________________________________________________________________________________________\nattention (Attention)           ((None, 10, 200), (N 40000       bidirectional[0][0]              \n                                                                 bidirectional[0][1]              \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 2000)         0           attention[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 5487)         10979487    flatten[0][0]                    \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 5487)         0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 2)            0           activation[0][0]                 \n==================================================================================================\nTotal params: 11,342,689\nTrainable params: 11,342,689\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"e=[]\nf=[]\ng=[]\nfor i in range(99):\n    f.append(0)\n    e.append(0)\n    g.append(0)","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class MyCallback(tf.keras.callbacks.Callback):\n    def on_train_end(self, logs=None):\n        global training_finished\n        training_finished = True\n    def on_epoch_end(self, epoch, logs=None):\n        z=len(e)\n        if z>len(g):\n            a=model.history.history['val_loss'][z-len(g)-1]\n            b=model.history.history['loss'][z-len(g)-1]\n            c=float(a)-float(b)\n            if c<0:\n                print(\"Sebelumnya Underfitting\")\n            else:\n                print(\"Sebelumnya Overfitting\")\n        e.append(0)\n            ","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class CustCallback(tf.keras.callbacks.Callback):\n    def on_train_end(self, logs=None):\n        global training_finished\n        training_finished = True\n    def on_epoch_end(self, epoch, logs=None):\n        z=len(f)\n        file_path=\"../working/Newweights\"+str(z)\n        weight=model.get_weights()\n        np.save(file_path, weight)\n        f.append(0)","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"file='../input/latbl91gpslocm/Newweights'+str(len(g)-1)+'.npy'\n#file='../input/dataset91/Newweights'+str(len(g)-1)+'.npy'\nb=np.load(file, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')\nmodel.set_weights(b)","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"file='../input/dataset91/train-labels.npy'\ntrain_labels=np.load(file, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')\nfile='../input/dataset91/validation-labels.npy'\nvalidation_labels=np.load(file, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, CSVLogger\nfrom keras.callbacks import EarlyStopping\n \ncallbacks = []\nif save_prefix is not None:\n        # Save the model's intermediary weights to disk after each epoch\n    file_path=\"cache/%s-{epoch:03d}-{val_loss:.4f}.hdf5\" % save_prefix\n    checkpoint = ModelCheckpoint(file_path,monitor='val_loss',mode='auto',save_weights_only=True,verbose=0)\n    #checkpoint = weight_save(model.get_weights(),b)\n    callbacks.append(checkpoint)\n    #callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', mode='min', save_weights_only=True, verbose=1))\n    #callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', verbose=0, save_best_only=False, mode='auto'))\n    #model.fit(X_train,y_train,batch_size=batch_size,nb_epoch=nb_epoch,callbacks=[weight_save_callback])\n#g.append(0)\n#epoch>1\n\n#first epoch\n#saver = weight_save(model.get_weights(),b)   \n#es = EarlyStopping(monitor='val_loss', patience=0, verbose=1)\narc='LATBiLSTMnew91-b200-'+str(len(g))+'.log'\ncsv_logger = CSVLogger(arc, separator=',', append=False)\nprint(\"Creating model...\")\nstart_new_session()\nprint(\"Train model...\")\nhistory=model.fit(train, train_labels,\n        initial_epoch=len(g),epochs=100, batch_size=batch_size,\n        validation_data=(validation, validation_labels)\n        ,callbacks=[csv_logger,CustCallback(),MyCallback()])","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Creating model...\n","output_type":"stream"},{"name":"stderr","text":"2022-01-05 16:19:23.385362: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"},{"name":"stdout","text":"Train model...\n","output_type":"stream"},{"name":"stderr","text":"2022-01-05 16:19:24.088237: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 100/100\n","output_type":"stream"},{"name":"stderr","text":"2022-01-05 16:19:27.927313: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"111\" frequency: 2200 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.3.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 57671680 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n","output_type":"stream"},{"name":"stdout","text":"7400/7400 [==============================] - ETA: 0s - loss: 1.3686","output_type":"stream"},{"name":"stderr","text":"2022-01-05 16:54:12.886785: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"111\" frequency: 2200 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.3.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 57671680 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n","output_type":"stream"},{"name":"stdout","text":"7400/7400 [==============================] - 2170s 293ms/step - loss: 1.3686 - val_loss: 1.4156\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order, subok=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"t=0\nwhile(t<1):\n    t=t","metadata":{"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/1653830578.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}