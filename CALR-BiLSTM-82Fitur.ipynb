{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"config=3\n# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/fp2021/data.py\", dst = \"../working/data.py\")\ncopyfile(src = \"../input/fp2021/utils.py\", dst = \"../working/utils.py\")\ncopyfile(src = \"../input/fp2021/training.py\", dst = \"../working/training.py\")\ncopyfile(src = \"../input/fp2021/layers.py\", dst = \"../working/layers.py\")\n#copyfile(src = \"../input/notes1/weights2.npy\", dst = \"../working/weights2.npy\")\n# import all our functions\n#from data import load_data","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:12.732834Z","iopub.execute_input":"2022-01-20T14:25:12.733176Z","iopub.status.idle":"2022-01-20T14:25:12.758445Z","shell.execute_reply.started":"2022-01-20T14:25:12.733143Z","shell.execute_reply":"2022-01-20T14:25:12.757130Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'../working/layers.py'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install pickle5\nimport pickle5 as pickle\nimport csv\nimport calendar\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import  MultipleLocator, FormatStrFormatter\nfrom scipy.interpolate import splrep\nfrom IPython.core.display import display_html\nfrom keras.models import load_model\nfrom utils import np_haversine, density_map, get_clusters, plot_embeddings\nfrom data import load_data\nfrom training import start_new_session, process_features, create_model\n\n# Display plots inline\n%matplotlib inline\n\n# Fix random seed for reproducibility\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:12.767067Z","iopub.execute_input":"2022-01-20T14:25:12.767416Z","iopub.status.idle":"2022-01-20T14:25:21.759415Z","shell.execute_reply.started":"2022-01-20T14:25:12.767385Z","shell.execute_reply":"2022-01-20T14:25:21.757899Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pickle5 in /opt/conda/lib/python3.7/site-packages (0.0.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"clusters_cache = '/kaggle/input/fp2021/clusters.npy'\nclusters=np.load(clusters_cache, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:21.767941Z","iopub.execute_input":"2022-01-20T14:25:21.768757Z","iopub.status.idle":"2022-01-20T14:25:21.787908Z","shell.execute_reply.started":"2022-01-20T14:25:21.768665Z","shell.execute_reply":"2022-01-20T14:25:21.786164Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"metadata_cache = '/kaggle/input/fp2021/metadata.pickle'\nwith open(metadata_cache, 'rb') as handle:\n    metadata = pickle.load(handle)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:21.794833Z","iopub.execute_input":"2022-01-20T14:25:21.798251Z","iopub.status.idle":"2022-01-20T14:25:21.809341Z","shell.execute_reply.started":"2022-01-20T14:25:21.798204Z","shell.execute_reply":"2022-01-20T14:25:21.808277Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"file='/kaggle/input/fp2021/train0.npy'\ntrain0=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train1.npy'\ntrain1=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train2.npy'\ntrain2=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train3.npy'\ntrain3=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train4.npy'\ntrain4=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train5.npy'\ntrain5=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train6.npy'\ntrain6=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\ntrain=[train0,train1,train2,train3,train4,train5,train6]","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:21.815029Z","iopub.execute_input":"2022-01-20T14:25:21.817235Z","iopub.status.idle":"2022-01-20T14:25:22.005178Z","shell.execute_reply.started":"2022-01-20T14:25:21.817187Z","shell.execute_reply":"2022-01-20T14:25:22.004145Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"file='/kaggle/input/fp2021/validation0.npy'\nvalidation0=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation1.npy'\nvalidation1=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation2.npy'\nvalidation2=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation3.npy'\nvalidation3=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation4.npy'\nvalidation4=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation5.npy'\nvalidation5=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation6.npy'\nvalidation6=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nvalidation=[validation0,validation1,validation2,validation3,validation4,validation5,validation6]","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:22.012594Z","iopub.execute_input":"2022-01-20T14:25:22.015124Z","iopub.status.idle":"2022-01-20T14:25:22.095293Z","shell.execute_reply.started":"2022-01-20T14:25:22.015044Z","shell.execute_reply":"2022-01-20T14:25:22.094273Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"metadata_cache = '/kaggle/input/fp2021/metadata.pickle'\nwith open(metadata_cache, 'rb') as handle:\n    metadata = pickle.load(handle)\ntl_cache = '/kaggle/input/fp2021/train-labels.npy'\nvl_cache = '/kaggle/input/fp2021/validation-labels.npy'\ntrain_labels=np.load(tl_cache, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')\nvalidation_labels=np.load(vl_cache, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:22.101140Z","iopub.execute_input":"2022-01-20T14:25:22.103669Z","iopub.status.idle":"2022-01-20T14:25:22.139125Z","shell.execute_reply.started":"2022-01-20T14:25:22.103624Z","shell.execute_reply":"2022-01-20T14:25:22.138267Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#import pickle\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import scale\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import SGD, Adam, Adagrad\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.core import Dense, Reshape, Activation, Dropout\nfrom keras.layers import *\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom utils import tf_haversine\nfrom data import load_data\nfrom utils import get_clusters\nfrom keras.models import Model\n\nfrom tensorflow.compat.v1.keras.layers import LSTM   # CuDNNLSTM\nimport sys\nsys.path.append('..')  # add parent directory to Python path for layers.py access\nfrom layers import Attention, SelfAttention\n\ndef create_model(metadata,clusters,config):\n    \"\"\"\n    Creates all the layers for our neural network model.\n    \"\"\"\n      \n    # Arbitrary dimension for all embeddings\n    embedding_dim = 10\n\n    # Quarter hour of the day embedding\n    embed_quarter_hour = Sequential()\n    embed_quarter_hour.add(Embedding(metadata['n_quarter_hours'], embedding_dim, input_length=1))\n    embed_quarter_hour.add(Reshape((embedding_dim,1)))\n\n    # Day of the week embedding\n    embed_day_of_week = Sequential()\n    embed_day_of_week.add(Embedding(metadata['n_days_per_week'], embedding_dim, input_length=1))\n    embed_day_of_week.add(Reshape((embedding_dim,1)))\n\n    # Week of the year embedding\n    embed_week_of_year = Sequential()\n    embed_week_of_year.add(Embedding(metadata['n_weeks_per_year'], embedding_dim, input_length=1))\n    embed_week_of_year.add(Reshape((embedding_dim,1)))\n\n    # Client ID embedding\n    embed_client_ids = Sequential()\n    embed_client_ids.add(Embedding(metadata['n_client_ids'], embedding_dim, input_length=1))\n    embed_client_ids.add(Reshape((embedding_dim,1)))\n\n    # Taxi ID embedding\n    embed_taxi_ids = Sequential()\n    embed_taxi_ids.add(Embedding(metadata['n_taxi_ids'], embedding_dim, input_length=1))\n    embed_taxi_ids.add(Reshape((embedding_dim,1)))\n\n    # Taxi stand ID embedding\n    embed_stand_ids = Sequential()\n    embed_stand_ids.add(Embedding(metadata['n_stand_ids'], embedding_dim, input_length=1))\n    embed_stand_ids.add(Reshape((embedding_dim,1)))\n    \n    # GPS coordinates (5 first lat/long and 5 latest lat/long, therefore 20 values)\n    coords = Sequential()\n    coords.add(Dense(1, input_dim=20))\n\n    # Merge all the inputs into a single input layer\n    mergedOut = Add()([embed_quarter_hour.output,\n                embed_day_of_week.output,\n                embed_week_of_year.output,\n                embed_client_ids.output,\n                embed_taxi_ids.output,\n                embed_stand_ids.output,\n                coords.output])\n    \n    #encoder_output, hidden_state, cell_state = LSTM(50, activation='tanh',input_shape=(None,None, 1),\n    #                                                return_sequences=True,return_state=True)(mergedOut)\n    #attention_input = [encoder_output, hidden_state]\n    lstm_layer = tf.keras.layers.LSTM(200, return_sequences=True,return_state=True)\n    if config != 0:\n        encoder_output, hidden_state, cell_state,  *_ = Bidirectional(lstm_layer,merge_mode=\"sum\")(mergedOut)\n        attention_input = [encoder_output, hidden_state]\n    else:\n        encoder_output = LSTM(units=64)(mergedOut)\n\n    # Optional Attention Mechanisms\n    if config == 1:\n        encoder_output, attention_weights = SelfAttention(size=64,\n                                                      num_hops=10,\n                                                      use_penalization=False)(encoder_output)\n    elif config == 2:\n        encoder_output, attention_weights = Attention(context='many-to-one',\n                                                  alignment_type='global')(attention_input)\n        encoder_output = Flatten()(encoder_output)\n    elif config == 3:\n        encoder_output, attention_weights = Attention(context='many-to-one',\n                                                  alignment_type='local-m',\n                                                  window_width=10,\n                                                  score_function='general')(attention_input)\n        encoder_output = Flatten()(encoder_output)\n        \n    \n    #encoder_output=Dense(1)(encoder_output)\n    # Determine cluster probabilities using softmax\n    mergedOut=Dense(len(clusters))(encoder_output)\n    mergedOut=Activation('softmax')(mergedOut)\n\n    # Final activation layer: calculate the destination as the weighted mean of cluster coordinates\n    cast_clusters = K.cast_to_floatx(clusters)\n    def destination(probabilities):\n        return tf.matmul(probabilities, cast_clusters)\n    mergedOut=Activation(destination)(mergedOut)\n\n    newModel = Model([embed_quarter_hour.input,\n                embed_day_of_week.input,\n                embed_week_of_year.input,\n                embed_client_ids.input,\n                embed_taxi_ids.input,\n                embed_stand_ids.input,\n                coords.input], mergedOut)\n    #use lists if you want more than one input or output  \n    \n    # Compile the model\n    optimizer = SGD(lr=0.1, momentum=0.9, clipvalue=1.)  # Use `clipvalue` to prevent exploding gradients\n\n    newModel.compile(loss=tf_haversine, optimizer=optimizer)\n    \n    return newModel","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:22.145082Z","iopub.execute_input":"2022-01-20T14:25:22.149384Z","iopub.status.idle":"2022-01-20T14:25:22.182712Z","shell.execute_reply.started":"2022-01-20T14:25:22.149305Z","shell.execute_reply":"2022-01-20T14:25:22.181392Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model=create_model(metadata,clusters,config)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:22.184525Z","iopub.execute_input":"2022-01-20T14:25:22.185013Z","iopub.status.idle":"2022-01-20T14:25:22.918310Z","shell.execute_reply.started":"2022-01-20T14:25:22.184898Z","shell.execute_reply":"2022-01-20T14:25:22.917315Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class CustCallback(tf.keras.callbacks.Callback):\n    def on_train_end(self, logs=None):\n        global training_finished\n        training_finished = True\n    def on_epoch_end(self, epoch, logs=None):\n        z=len(f)\n        file_path=\"../working/Newweights\"+str(z)\n        weight=model.get_weights()\n        np.save(file_path, weight)\n        f.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:22.919873Z","iopub.execute_input":"2022-01-20T14:25:22.920166Z","iopub.status.idle":"2022-01-20T14:25:22.927094Z","shell.execute_reply.started":"2022-01-20T14:25:22.920124Z","shell.execute_reply":"2022-01-20T14:25:22.926014Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class MyCallback(tf.keras.callbacks.Callback):\n    def on_train_end(self, logs=None):\n        global training_finished\n        training_finished = True\n    def on_epoch_end(self, epoch, logs=None):\n        z=len(e)\n        if z>len(g):\n            a=model.history.history['val_loss'][z-len(g)-1]\n            b=model.history.history['loss'][z-len(g)-1]\n            c=float(a)-float(b)\n            if c<0:\n                print(\"Sebelumnya Underfitting\")\n            else:\n                print(\"Sebelumnya Overfitting\")\n        e.append(0)\n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:22.928866Z","iopub.execute_input":"2022-01-20T14:25:22.929637Z","iopub.status.idle":"2022-01-20T14:25:22.940786Z","shell.execute_reply.started":"2022-01-20T14:25:22.929575Z","shell.execute_reply":"2022-01-20T14:25:22.939583Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"e=[]\nf=[]\ng=[]\nh=[]\nfor i in range(80):\n    f.append(0)\n    e.append(0)\n    g.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:22.942596Z","iopub.execute_input":"2022-01-20T14:25:22.943137Z","iopub.status.idle":"2022-01-20T14:25:22.959111Z","shell.execute_reply.started":"2022-01-20T14:25:22.943065Z","shell.execute_reply":"2022-01-20T14:25:22.958239Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#file='../input/latb82fiturlocm/Newweights'+str(len(g)-1)+'.npy'\nfile='./Newweights'+str(len(g)-1)+'.npy'\ny=np.load(file,allow_pickle=True)\nmodel.set_weights(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:22.962503Z","iopub.execute_input":"2022-01-20T14:25:22.962739Z","iopub.status.idle":"2022-01-20T14:25:23.089085Z","shell.execute_reply.started":"2022-01-20T14:25:22.962710Z","shell.execute_reply":"2022-01-20T14:25:23.087946Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Estimate the GPS clusters\nprint(\"Estimating clusters...\")\n#clusters = get_clusters(data.train_labels)\nn_epochs=100\nbatch_size=200\nsave_prefix='mymodel'\n","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:23.091054Z","iopub.execute_input":"2022-01-20T14:25:23.091469Z","iopub.status.idle":"2022-01-20T14:25:23.098542Z","shell.execute_reply.started":"2022-01-20T14:25:23.091400Z","shell.execute_reply":"2022-01-20T14:25:23.097417Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Estimating clusters...\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, CSVLogger\nfrom keras.callbacks import EarlyStopping\n \ncallbacks = []\nif save_prefix is not None:\n        # Save the model's intermediary weights to disk after each epoch\n    file_path=\"cache/%s-{epoch:03d}-{val_loss:.4f}.hdf5\" % save_prefix\n    checkpoint = ModelCheckpoint(file_path,monitor='val_loss',mode='auto',save_weights_only=True,verbose=0)\n    #checkpoint = weight_save(model.get_weights(),b)\n    callbacks.append(checkpoint)\n    #callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', mode='min', save_weights_only=True, verbose=1))\n    #callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', verbose=0, save_best_only=False, mode='auto'))\n    #model.fit(X_train,y_train,batch_size=batch_size,nb_epoch=nb_epoch,callbacks=[weight_save_callback])\n#g.append(0)\n#epoch>1\n#first epoch\n#saver = weight_save(model.get_weights(),b)   \n#es = EarlyStopping(monitor='val_loss', patience=0, verbose=1)\narc='AttenLSTMfitur91-b200-'+str(len(g))+'.log'\ncsv_logger = CSVLogger(arc, separator=',', append=False)\nprint(\"Creating model...\")\nstart_new_session()\nprint(\"Train model...\")\nhistory=model.fit(train, train_labels,\n        initial_epoch=len(g),epochs=100, batch_size=batch_size,\n        validation_data=(validation, validation_labels)\n        ,callbacks=[csv_logger,CustCallback(),MyCallback()])\nt=0\nwhile(t<1):\n    t=t","metadata":{"execution":{"iopub.status.busy":"2022-01-20T14:25:23.103784Z","iopub.execute_input":"2022-01-20T14:25:23.104545Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Creating model...\nTrain model...\n","output_type":"stream"},{"name":"stderr","text":"2022-01-20 14:25:23.114215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-20 14:25:23.115589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-20 14:25:23.116513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-20 14:25:23.117693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-20 14:25:23.118840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-20 14:25:23.119716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 81/100\n","output_type":"stream"},{"name":"stderr","text":"2022-01-20 14:25:27.362480: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla P100-PCIE-16GB\" frequency: 1328 num_cores: 56 environment { key: \"architecture\" value: \"6.0\" } environment { key: \"cuda\" value: \"11000\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 16152002560 bandwidth: 732160000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n","output_type":"stream"},{"name":"stdout","text":"6577/6578 [============================>.] - ETA: 0s - loss: 1.9024","output_type":"stream"},{"name":"stderr","text":"2022-01-20 14:27:06.659498: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla P100-PCIE-16GB\" frequency: 1328 num_cores: 56 environment { key: \"architecture\" value: \"6.0\" } environment { key: \"cuda\" value: \"11000\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 16152002560 bandwidth: 732160000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n","output_type":"stream"},{"name":"stdout","text":"6578/6578 [==============================] - 113s 17ms/step - loss: 1.9024 - val_loss: 2.0095\nEpoch 82/100\n6578/6578 [==============================] - 109s 17ms/step - loss: 1.9005 - val_loss: 2.0061\nSebelumnya Overfitting\nEpoch 83/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8997 - val_loss: 2.0176\nSebelumnya Overfitting\nEpoch 84/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8982 - val_loss: 2.0099\nSebelumnya Overfitting\nEpoch 85/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8978 - val_loss: 2.0199\nSebelumnya Overfitting\nEpoch 86/100\n6578/6578 [==============================] - 108s 16ms/step - loss: 1.8962 - val_loss: 2.0058\nSebelumnya Overfitting\nEpoch 87/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8959 - val_loss: 2.0078\nSebelumnya Overfitting\nEpoch 88/100\n6578/6578 [==============================] - 108s 16ms/step - loss: 1.8958 - val_loss: 2.0048\nSebelumnya Overfitting\nEpoch 89/100\n6578/6578 [==============================] - 109s 17ms/step - loss: 1.8948 - val_loss: 2.0161\nSebelumnya Overfitting\nEpoch 90/100\n6578/6578 [==============================] - 108s 16ms/step - loss: 1.8936 - val_loss: 2.0182\nSebelumnya Overfitting\nEpoch 91/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8933 - val_loss: 2.0107\nSebelumnya Overfitting\nEpoch 92/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8904 - val_loss: 2.0126\nSebelumnya Overfitting\nEpoch 93/100\n6578/6578 [==============================] - 108s 16ms/step - loss: 1.8912 - val_loss: 2.0183\nSebelumnya Overfitting\nEpoch 94/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8905 - val_loss: 2.0116\nSebelumnya Overfitting\nEpoch 95/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8876 - val_loss: 2.0115\nSebelumnya Overfitting\nEpoch 96/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8877 - val_loss: 2.0059\nSebelumnya Overfitting\nEpoch 97/100\n6578/6578 [==============================] - 108s 16ms/step - loss: 1.8862 - val_loss: 2.0206\nSebelumnya Overfitting\nEpoch 98/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8858 - val_loss: 2.0182\nSebelumnya Overfitting\nEpoch 99/100\n6578/6578 [==============================] - 107s 16ms/step - loss: 1.8838 - val_loss: 2.0125\nSebelumnya Overfitting\nEpoch 100/100\n6578/6578 [==============================] - 106s 16ms/step - loss: 1.8838 - val_loss: 2.0269\nSebelumnya Overfitting\n","output_type":"stream"}]},{"cell_type":"code","source":"#metadata_cache = '/kaggle/input/dataset91/train.pickle'\n#with open(metadata_cache, 'rb') as handle:\n#    train = pickle.load(handle)\n#metadata_cache = '/kaggle/input/dataset91/validation.pickle'\n#with open(metadata_cache, 'rb') as handle:\n#    validation = pickle.load(handle)\nimport pickle5 as pickle\nmetadata_cache = '/kaggle/input/fp2021/competition-test.pickle'\nwith open(metadata_cache, 'rb') as handle:\n    competition_test = pickle.load(handle)\ntrain_labels=np.load('/kaggle/input/fp2021/train-labels.npy')\nvalidation_labels=np.load('/kaggle/input/fp2021/validation-labels.npy')\ncompetition_test_labels=np.load('/kaggle/input/fp2021/competition-test-labels.npy')\nprint(\"HASIL EVALUASI MODEL-INPUT 82 Full GPS\")\nprint(\" \")\ntest_predictions = model.predict(process_features(competition_test))\nprint(\"Mean Haversine Test:{}\".format(np_haversine(test_predictions, competition_test_labels).mean()))\nvalidation_predictions = model.predict(validation)\nprint(\"Mean Haversine Validation:{}\".format(np_haversine(validation_predictions, validation_labels).mean()))\ntrain_predictions = model.predict(train)\nprint(\"Mean Haversine Train:{}\".format(np_haversine(train_predictions, train_labels).mean()))\nprint(\" \")\ny_train=train_labels\ny_train_pred=train_predictions\ny_valid=validation_labels\ny_valid_pred=validation_predictions\ny_test=competition_test_labels\ny_test_pred=test_predictions\nfrom sklearn import metrics\nprint(\"MSE train:{}\".format(metrics.mean_squared_error(y_train, y_train_pred)))\nprint(\"MSE validation:{}\".format(metrics.mean_squared_error(y_valid, y_valid_pred)))\nprint(\"MSE test:{}\".format(metrics.mean_squared_error(y_test, y_test_pred)))\nprint(\" \")\nprint(\"MAE score train:{}\".format(metrics.mean_absolute_error(y_train, y_train_pred)))\nprint(\"MAE score validation:{}\".format(metrics.mean_absolute_error(y_valid, y_valid_pred)))\nprint(\"MAE score test:{}\".format(metrics.mean_absolute_error(y_test, y_test_pred)))\nprint(\" \")\nprint(\"R2 score train:{}\".format(metrics.r2_score(y_train, y_train_pred)))\nprint(\"R2 score validation:{}\".format(metrics.r2_score(y_valid, y_valid_pred)))\nprint(\"R2 score test:{}\".format(metrics.r2_score(y_test, y_test_pred)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}