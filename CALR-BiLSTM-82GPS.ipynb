{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/fp2021/data.py\", dst = \"../working/data.py\")\ncopyfile(src = \"../input/fp2021/utils.py\", dst = \"../working/utils.py\")\ncopyfile(src = \"../input/fp2021/training.py\", dst = \"../working/training.py\")\ncopyfile(src = \"../input/fp2021/layers.py\", dst = \"../working/layers.py\")\n#copyfile(src = \"../input/notes1/weights2.npy\", dst = \"../working/weights2.npy\")\n# import all our functions\n#from data import load_data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-22T02:23:04.825731Z","iopub.execute_input":"2022-01-22T02:23:04.826001Z","iopub.status.idle":"2022-01-22T02:23:04.847978Z","shell.execute_reply.started":"2022-01-22T02:23:04.825964Z","shell.execute_reply":"2022-01-22T02:23:04.847156Z"},"trusted":true},"execution_count":161,"outputs":[{"execution_count":161,"output_type":"execute_result","data":{"text/plain":"'../working/layers.py'"},"metadata":{}}]},{"cell_type":"code","source":"# Estimate the GPS clusters\nprint(\"Estimating clusters...\")\n#clusters = get_clusters(data.train_labels)\nn_epochs=100\nbatch_size=200\nsave_prefix='mymodel'\nconfig=3\n","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:04.850021Z","iopub.execute_input":"2022-01-22T02:23:04.850390Z","iopub.status.idle":"2022-01-22T02:23:04.855729Z","shell.execute_reply.started":"2022-01-22T02:23:04.850353Z","shell.execute_reply":"2022-01-22T02:23:04.854821Z"},"trusted":true},"execution_count":162,"outputs":[{"name":"stdout","text":"Estimating clusters...\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pickle5\nimport pickle5 as pickle\nimport csv\nimport calendar\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import  MultipleLocator, FormatStrFormatter\nfrom scipy.interpolate import splrep\nfrom IPython.core.display import display_html\nfrom keras.models import load_model\nfrom utils import np_haversine, density_map, get_clusters, plot_embeddings\nfrom data import load_data\nfrom training import start_new_session, process_features, create_model\n\n# Display plots inline\n%matplotlib inline\n\n# Fix random seed for reproducibility\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:04.857364Z","iopub.execute_input":"2022-01-22T02:23:04.857626Z","iopub.status.idle":"2022-01-22T02:23:12.274321Z","shell.execute_reply.started":"2022-01-22T02:23:04.857591Z","shell.execute_reply":"2022-01-22T02:23:12.273404Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pickle5 in /opt/conda/lib/python3.7/site-packages (0.0.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"clusters_cache = '/kaggle/input/fp2021/clusters.npy'\nclusters=np.load(clusters_cache, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:12.277087Z","iopub.execute_input":"2022-01-22T02:23:12.277566Z","iopub.status.idle":"2022-01-22T02:23:12.288074Z","shell.execute_reply.started":"2022-01-22T02:23:12.277524Z","shell.execute_reply":"2022-01-22T02:23:12.287333Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"file='/kaggle/input/fp2021/train0.npy'\ntrain0=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train1.npy'\ntrain1=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train2.npy'\ntrain2=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train3.npy'\ntrain3=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train4.npy'\ntrain4=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train5.npy'\ntrain5=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/train6.npy'\ntrain6=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\ntrain=[train0,train1,train2,train3,train4,train5,train6]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:12.289675Z","iopub.execute_input":"2022-01-22T02:23:12.290339Z","iopub.status.idle":"2022-01-22T02:23:12.404173Z","shell.execute_reply.started":"2022-01-22T02:23:12.290296Z","shell.execute_reply":"2022-01-22T02:23:12.403388Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"file='/kaggle/input/fp2021/validation0.npy'\nvalidation0=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation1.npy'\nvalidation1=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation2.npy'\nvalidation2=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation3.npy'\nvalidation3=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation4.npy'\nvalidation4=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation5.npy'\nvalidation5=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nfile='/kaggle/input/fp2021/validation6.npy'\nvalidation6=np.load(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')\nvalidation=[validation0,validation1,validation2,validation3,validation4,validation5,validation6]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:12.406238Z","iopub.execute_input":"2022-01-22T02:23:12.406677Z","iopub.status.idle":"2022-01-22T02:23:12.458846Z","shell.execute_reply.started":"2022-01-22T02:23:12.406637Z","shell.execute_reply":"2022-01-22T02:23:12.458079Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"!pip3 install pickle5\nimport pickle5 as pickle\nmetadata_cache = '/kaggle/input/fp2021/metadata.pickle'\nwith open(metadata_cache, 'rb') as handle:\n    metadata = pickle.load(handle)\ntl_cache = '/kaggle/input/fp2021/train-labels.npy'\nvl_cache = '/kaggle/input/fp2021/validation-labels.npy'\ntrain_labels=np.load(tl_cache, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')\nvalidation_labels=np.load(vl_cache, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:12.460387Z","iopub.execute_input":"2022-01-22T02:23:12.460668Z","iopub.status.idle":"2022-01-22T02:23:20.329292Z","shell.execute_reply.started":"2022-01-22T02:23:12.460633Z","shell.execute_reply":"2022-01-22T02:23:20.328407Z"},"trusted":true},"execution_count":167,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pickle5 in /opt/conda/lib/python3.7/site-packages (0.0.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"train=train[6]\nvalidation=validation[6]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:20.331139Z","iopub.execute_input":"2022-01-22T02:23:20.331400Z","iopub.status.idle":"2022-01-22T02:23:20.335967Z","shell.execute_reply.started":"2022-01-22T02:23:20.331363Z","shell.execute_reply":"2022-01-22T02:23:20.335298Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"def first_last_k(coords):\n    \"\"\"\n    Returns a list with the first k and last k GPS coordinates from the given trip.\n    The returned list contains 4k values (latitudes and longitudes for 2k points).\n    \"\"\"\n    k = 5\n    partial = [coords[0] for i in range(2*k)]\n    num_coords = len(coords)\n    if num_coords < 2*k:\n        partial[-num_coords:] = coords\n    else:\n        partial[:k] = coords[:k]\n        partial[-k:] = coords[-k:]\n    partial = np.row_stack(partial)\n    return np.array(partial).flatten()\n\n\ndef process_features(df):\n    \"\"\"\n    Process the features required by our model from the given dataframe.\n    Return the features in a list so that they can be merged in our model's input layer.\n    \"\"\"\n    # Fetch the first and last GPS coordinates\n    coords = np.row_stack(df['POLYLINE'].apply(first_last_k))\n    # Standardize latitudes (odd columns) and longitudes (even columns)\n    latitudes = coords[:,::2]\n    coords[:,::2] = scale(latitudes)\n    longitudes = coords[:,1::2]\n    coords[:,1::2] = scale(longitudes)\n    \n    return [\n        coords\n    ]\n\n\nfrom tensorflow.compat.v1.keras.layers import LSTM   # CuDNNLSTM\nimport sys\nsys.path.append('..')  # add parent directory to Python path for layers.py access\nfrom layers import Attention, SelfAttention\n\n#trayektori\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import scale\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import SGD, Adam, Adagrad\nimport numpy as np\nfrom keras.layers.core import Dense, Reshape, Activation, Dropout\nfrom keras.layers import *\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom utils import tf_haversine\n\nfrom utils import get_clusters\nfrom keras.models import Model\n\ndef create_model(metadata,clusters,config):\n    from layers import Attention, SelfAttention\n    \"\"\"\n    Creates all the layers for our neural network model.\n    \"\"\"\n      \n    # Arbitrary dimension for all embeddings\n    embedding_dim = 10\n\n    # GPS coordinates (5 first lat/long and 5 latest lat/long, therefore 20 values)\n    coords = Sequential()\n    coords.add(Dense(1, input_dim=20,input_shape=(20,1)))\n    \n    #np.expand_dims(coords,axis=0)\n    mergedOut = coords.output\n\n    lstm_layer = tf.keras.layers.LSTM(200, return_sequences=True,return_state=True)\n    if config != 0:\n        encoder_output, hidden_state, cell_state,  *_ = Bidirectional(lstm_layer,merge_mode=\"sum\")(mergedOut)\n        attention_input = [encoder_output, hidden_state]\n    else:\n        encoder_output = LSTM(units=64)(mergedOut)\n\n    # Optional Attention Mechanisms\n    if config == 1:\n        encoder_output, attention_weights = SelfAttention(size=64,\n                                                      num_hops=10,\n                                                      use_penalization=False)(encoder_output)\n    elif config == 2:\n        encoder_output, attention_weights = Attention(context='many-to-one',\n                                                  alignment_type='global')(attention_input)\n        encoder_output = Flatten()(encoder_output)\n    elif config == 3:\n        encoder_output, attention_weights = Attention(context='many-to-one',\n                                                  alignment_type='local-m',\n                                                  window_width=10,\n                                                  score_function='general')(attention_input)\n        encoder_output = Flatten()(encoder_output)\n        \n    \n    #encoder_output=Dense(1)(encoder_output)\n    # Determine cluster probabilities using softmax\n    mergedOut=Dense(len(clusters))(encoder_output)\n    mergedOut=Activation('softmax')(mergedOut)\n\n    # Final activation layer: calculate the destination as the weighted mean of cluster coordinates\n    cast_clusters = K.cast_to_floatx(clusters)\n    def destination(probabilities):\n        return tf.matmul(probabilities, cast_clusters)\n    mergedOut=Activation(destination)(mergedOut)\n\n    newModel = Model([coords.input],mergedOut)\n    #use lists if you want more than one input or output  \n    \n    # Compile the model\n    optimizer = SGD(lr=0.01, momentum=0.9, clipvalue=1.)  # Use `clipvalue` to prevent exploding gradients\n\n    newModel.compile(loss=tf_haversine, optimizer=optimizer)\n    \n    return newModel","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:20.337600Z","iopub.execute_input":"2022-01-22T02:23:20.338130Z","iopub.status.idle":"2022-01-22T02:23:20.360041Z","shell.execute_reply.started":"2022-01-22T02:23:20.337966Z","shell.execute_reply":"2022-01-22T02:23:20.359268Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"model=create_model(metadata,clusters,config)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:20.363388Z","iopub.execute_input":"2022-01-22T02:23:20.363706Z","iopub.status.idle":"2022-01-22T02:23:20.847609Z","shell.execute_reply.started":"2022-01-22T02:23:20.363675Z","shell.execute_reply":"2022-01-22T02:23:20.846855Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"e=[]\nf=[]\ng=[]\nfor i in range(98):\n    f.append(0)\n    e.append(0)\n    g.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:20.849039Z","iopub.execute_input":"2022-01-22T02:23:20.849312Z","iopub.status.idle":"2022-01-22T02:23:20.854032Z","shell.execute_reply.started":"2022-01-22T02:23:20.849275Z","shell.execute_reply":"2022-01-22T02:23:20.853356Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"class MyCallback(tf.keras.callbacks.Callback):\n    def on_train_end(self, logs=None):\n        global training_finished\n        training_finished = True\n    def on_epoch_end(self, epoch, logs=None):\n        z=len(e)\n        if z>len(g):\n            a=model.history.history['val_loss'][z-len(g)-1]\n            b=model.history.history['loss'][z-len(g)-1]\n            c=float(a)-float(b)\n            if c<0:\n                print(\"Sebelumnya Underfitting\")\n            else:\n                print(\"Sebelumnya Overfitting\")\n        e.append(0)\n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:20.855512Z","iopub.execute_input":"2022-01-22T02:23:20.856034Z","iopub.status.idle":"2022-01-22T02:23:20.867862Z","shell.execute_reply.started":"2022-01-22T02:23:20.855994Z","shell.execute_reply":"2022-01-22T02:23:20.867144Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"class CustCallback(tf.keras.callbacks.Callback):\n    def on_train_end(self, logs=None):\n        global training_finished\n        training_finished = True\n    def on_epoch_end(self, epoch, logs=None):\n        z=len(f)\n        file_path=\"../working/Newweights\"+str(z)\n        weight=model.get_weights()\n        np.save(file_path, weight)\n        f.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:20.869439Z","iopub.execute_input":"2022-01-22T02:23:20.869736Z","iopub.status.idle":"2022-01-22T02:23:20.877758Z","shell.execute_reply.started":"2022-01-22T02:23:20.869699Z","shell.execute_reply":"2022-01-22T02:23:20.877059Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"#file='../input/latb82gpslocm/Newweights'+str(len(g)-1)+'.npy'\nfile='./Newweights'+str(len(g)-1)+'.npy'\nb=np.load(file, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')\nmodel.set_weights(b)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:20.878721Z","iopub.execute_input":"2022-01-22T02:23:20.879466Z","iopub.status.idle":"2022-01-22T02:23:20.998808Z","shell.execute_reply.started":"2022-01-22T02:23:20.879434Z","shell.execute_reply":"2022-01-22T02:23:20.998036Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"file='../input/fp2021/train-labels.npy'\ntrain_labels=np.load(file, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')\nfile='../input/fp2021/validation-labels.npy'\nvalidation_labels=np.load(file, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:21.001905Z","iopub.execute_input":"2022-01-22T02:23:21.002117Z","iopub.status.idle":"2022-01-22T02:23:21.021133Z","shell.execute_reply.started":"2022-01-22T02:23:21.002078Z","shell.execute_reply":"2022-01-22T02:23:21.020397Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, CSVLogger\nfrom keras.callbacks import EarlyStopping\n \ncallbacks = []\nif save_prefix is not None:\n        # Save the model's intermediary weights to disk after each epoch\n    file_path=\"cache/%s-{epoch:03d}-{val_loss:.4f}.hdf5\" % save_prefix\n    checkpoint = ModelCheckpoint(file_path,monitor='val_loss',mode='auto',save_weights_only=True,verbose=0)\n    #checkpoint = weight_save(model.get_weights(),b)\n    callbacks.append(checkpoint)\n    #callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', mode='min', save_weights_only=True, verbose=1))\n    #callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', verbose=0, save_best_only=False, mode='auto'))\n    #model.fit(X_train,y_train,batch_size=batch_size,nb_epoch=nb_epoch,callbacks=[weight_save_callback])\n#g.append(0)\n#epoch>1\n\n#first epoch\n#saver = weight_save(model.get_weights(),b)   \n#es = EarlyStopping(monitor='val_loss', patience=0, verbose=1)\narc='LSTMnew91-b200-'+str(len(g))+'.log'\ncsv_logger = CSVLogger(arc, separator=',', append=False)\nprint(\"Creating model...\")\nstart_new_session()\nprint(\"Train model...\")\nhistory=model.fit(train, train_labels,\n        initial_epoch=len(g),epochs=100, batch_size=batch_size,\n        validation_data=(validation, validation_labels)\n        ,callbacks=[csv_logger,CustCallback(),MyCallback()])\nt=0\nwhile(t<1):\n    t=t","metadata":{"execution":{"iopub.status.busy":"2022-01-22T02:23:21.024061Z","iopub.execute_input":"2022-01-22T02:23:21.024298Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Creating model...\nTrain model...\n","output_type":"stream"},{"name":"stderr","text":"2022-01-22 02:23:21.030066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 02:23:21.030957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 02:23:21.031552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 02:23:21.032205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 02:23:21.032745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 02:23:21.033214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 99/100\n","output_type":"stream"},{"name":"stderr","text":"2022-01-22 02:23:57.499883: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla P100-PCIE-16GB\" frequency: 1328 num_cores: 56 environment { key: \"architecture\" value: \"6.0\" } environment { key: \"cuda\" value: \"11000\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 16152002560 bandwidth: 732160000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n","output_type":"stream"},{"name":"stdout","text":"6575/6578 [============================>.] - ETA: 0s - loss: 1.3711","output_type":"stream"},{"name":"stderr","text":"2022-01-22 02:25:22.695364: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla P100-PCIE-16GB\" frequency: 1328 num_cores: 56 environment { key: \"architecture\" value: \"6.0\" } environment { key: \"cuda\" value: \"11000\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 16152002560 bandwidth: 732160000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n","output_type":"stream"},{"name":"stdout","text":"6578/6578 [==============================] - 130s 14ms/step - loss: 1.3711 - val_loss: 1.4038\nEpoch 100/100\n6578/6578 [==============================] - 92s 14ms/step - loss: 1.3702 - val_loss: 1.4064\nSebelumnya Overfitting\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle5 as pickle\nmetadata_cache = '/kaggle/input/fp2021/competition-test.pickle'\nwith open(metadata_cache, 'rb') as handle:\n    test = pickle.load(handle)\ntest_labels=np.load('/kaggle/input/fp2021/competition-test-labels.npy')\ntest_predictions = model.predict(process_features(test))\nprint(\"Mean Haversine Test:{}\".format(np_haversine(test_predictions, test_labels).mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}